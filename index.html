<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Eric Wang">

    <title>Eric Wang</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/ekko-lightbox.min.css" rel="stylesheet">
    <link href="slick/slick.css" rel="stylesheet">
    <link href="css/onepage.css" rel="stylesheet">
    

    <!-- Custom Fonts -->
    <link href="font-awesome-4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300' rel='stylesheet' type='text/css'>    

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" class="index">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">Eric Wang</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#about">about</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#project">projects</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#skills">skills</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#publication">publications</a>
                    </li>                                        
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

    <!-- Header -->
    <header id="myHeader" data-original="img/top-profile2.jpg">
        <div class="container">
            <div class="intro-text">
                <!--<div id="profile-wraper"></div>-->
                <div class="text-wraper">
                    <div class="intro-heading">Eric Wang<p style="font-size:0.5em;">(Cheng-Yao Wang)</p></div>
                    <div class="intro-lead-in">
                        <p>I’m Eric Wang, a Ph.D. student in Computer Science at Cornell University interested in <span class="about-focus">Human-Computer Interaction</span>, <span class="about-focus">Virtual Reality</span> and <span class="about-focus">Augmented Reality.</span></p>
                    </div>
                    <ul class="icon-list">
                        <li><a href="mailto:ericwang0701@gmail.com"><i class="fa fa-envelope-square fa-3x"></i></a></li>
                        <li><a href="https://www.facebook.com/profile.php?id=100000233739748" target="_blank"><i class="fa fa-facebook-square fa-3x"></i></a></li>
                        <li><a href="https://www.youtube.com/channel/UCbBZl_n5DbM4i8xhXJrdRIg" target="_blank"><i class="fa fa-youtube-square fa-3x"></i></a></li>
                        <li><a href="https://www.linkedin.com/in/ericwanghci" target="_blank"><i class="fa fa-linkedin-square fa-3x"></i></a></li>
                    </ul>
                </div>
            </div>
        </div>
    </header>

    <!-- About Section -->
    <section id="about">
        <div class="container">
            <h1>About me</h1>
            <div class="row text-left about-header">
                <div class="col-md-3 about-thumb">
                    <div id="profile-wraper"></div>
                </div>
                <div class="col-md-9 about-content">
                    <h3>I’m Eric Wang, a Ph.D. student in Computer Science at Cornell University advised by <a href="https://communication.cals.cornell.edu/people/andrea-won" target="_blank">Prof. Andrea Won</a> and <a href="https://www.cs.cornell.edu/~francois/" target="_blank">Prof. François Guimbretière</a>. Previously, I was a research intern at Stanford University, primarily working with <a href="https://profiles.stanford.edu/james-landay/" target="_blank">Prof. James Landay</a>. I have B.S and M.S. in Computer Science from National Taiwan University advised by <a href="http://mikechen.com/" target="_blank">Prof. Mike Y. Chen</a> and <a href="http://graphics.im.ntu.edu.tw/~robin/" target="_blank">Prof. Bing-Yu Chen</a>.</h3>

                    <h3>My current research interest is <span class="span-focus">Human-Computer Interaction</span> especially in <span class="span-focus">Virtaul Reality(VR)</span> and <span class="span-focus">Augmented Reality(AR)</span>. More specifically, I’m interested in utilizing VR and AR technologies to provide a better human-human, human-computer and human-robot interactions in different scenarios. Recently, I have developed a prototype system that allows people to capture all the details of a cherish memories and fully recreate the memory to re-experience it with their friends afterward. Unlike viewing photo or video, it provides a time machine-like experience which brings people back to the memory and people have full agency to view the experience from different perspectives and to paused, slowed down or rewound the experience, or intervene the past as well. I have started 2 projects to explore the potential of “Reliving Experience in VR ” in 2 different research areas including Social Interactions and Technology-Mediated Memory in VR (with <a href="https://communication.cals.cornell.edu/people/andrea-won" target="_blank">Prof. Andrea Won</a>) and Computer-Mediated Communication (with <a href="sfussell.hci.cornell.edu/" target="_blank">Prof. Susan Fussell</a>).</h3>

                    <h3>In terms of the research in AR, I have been working with <a href="https://www.huaishu.me/" target="_blank">Dr. Huaishu Peng</a> and Prof. François on the project - RoMA: Interactive Fabrication with Augmented Reality and a Robotic 3D Printer - which published at CHI 2018.  I believe AR technologies offer new opportunities for robots to communicate about their intents and the environments they share with human teammates. Thus, in the future, I also plan to explore whether people can build better trust with robot partners through visualizing explanations of robots' behavior in AR.</h3>
    
                </div>                
            </div>
            <div class="row text-left font-height">
                <div class="col-md-3">
                    <h2>Education</h2>
                </div>
                <div class="col-md-9 about-list">
                    <ul>
                        <li class="list-left"><h3><span class="bold">Ph.D.</span> in CS, Cornell University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Sep '16 – Current</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">M.S.</span> in CS, National Taiwan University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Sep '12 – Jun '14</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">B.S.</span> in CS, National Taiwan University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Feb '06 – Aug '10</h5></li>
                    </ul>                    
                </div>                               
            </div>
            <div class="row text-left font-height">
                <div class="col-md-3">
                    <h2>Experience</h2>
                </div>
                <div class="col-md-9 about-list">
                    <ul>
                        <li class="list-left"><h3><span class="bold">Research Assistant,</span>Project - Remote Touch for Telepresence Robots, Cornell University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Spring 2018</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Teaching Assistant,</span>CS 5306 Crowdsourcing and Human Computation, Cornell University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Fall 2017</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Teaching Assistant,</span>CS 1112 - Introduction to Computing using Matlab, Cornell University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Spring 2017</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Teaching Assistant,</span>CS 3110 - Data Structures and Functional Programming, Cornell University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Fall 2016</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Research Intern,</span>Human-Drone Interaction project, Stanford University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Apr '15 – Sep '15</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Chief Coordinator,</span><a href="https://drive.google.com/file/d/0B-TOzIcx7m_melhIZkRpWGtHd0NJOGNnQi15VVdod2kzbGZz/view?usp=sharing" class="link" target="_blank">2015 CHI@NTU Workshop</a></h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Aug '14 – Sep '14</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Teaching Assistant,</span><a href="https://mobilehci.hackpad.com/2014S-Mobile-HCI-Class-Schedule-wM4Izj9r0Oa" class="link" target="_blank">Mobile HCI</a>, National Taiwan University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Feb '14 – Jun '14</h5></li>
                    </ul>                
                </div>                                            
            </div>
            <p class="btn-download text-center"><a href="https://drive.google.com/open?id=1fuOInp46tMtEuwy1HiNejzNshdLQ0KK7" class="link" target="_blank"><i class="fa fa-download"></i>download cv</a></p>
        </div>
    </section>

    <section id="skill-bar">
        <div class="my-skill">            
            <ul>
                <li>
                    <h2>Computer Science</h2>
                    <p class="skill-img"><img class="lazy" data-original="img/skill1.png"></p>
                </li>
                <i class="fa fa-plus"></i>
                <li>
                    <h2>Hardware Prototyping</h2>
                    <p class="skill-img"><img class="lazy" data-original="img/skill2.png"></p>
                </li>
                <i class="fa fa-plus"></i>
                <li>
                    <h2>User Study Design</h2>
                    <p class="skill-img"><img class="lazy" data-original="img/skill3.png"></p>
                </li>
            </ul>            
            <div class="h-wraper"><h3>Novel Technologies Published in Top Conference and 1st Prize of Competitions</h3></div>
        </div>
    </section>

    <!-- project Section -->
    <section id="project">
        <div class="container">
            <h1>projects</h1>
            <!-- RelivingVR project-->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style small-padding">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/Asynchronous collaboration in VR_2.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/Asynchronous collaboration in VR.jpg"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/RelivingVRExperience.jpg"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/RelivingVRExperience2.png"></div>                      
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">On-going</header>
                        Understanding social interactions and emotional responses in capturing and reliving VR experience.
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang</span></h4>             
                    <h3>we present a prototype system that allows people to capture and relive memories in VR. While reliving the experience in VR with our prototype system, people have full agency to view the experience from different perspectives and to paused, slowed down or rewound the experience as well. More importantly, through our prototype system, we plan to design and conduct 2 user studies to explore the 2 important research questions: RQ1: How does reliving memories in VR affect our emotions and RQ2: What social interactions would occur while people relive experience with others</h3>
                    <ul class="list-btn">
                        <!--<li><a href="#workModal3" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>-->
                        <li><a href="https://www.youtube.com/edit?o=U&video_id=Mn4o5R_-ERI" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video 1</a></li>
                        <li><a href="https://www.youtube.com/watch?v=CG_qL5p_QEM" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video 2</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- RelivingVR project -->
            <!-- RoMA project-->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style small-padding">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/RoMA_img1.jpg"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/RoMA_img5.jpg"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/RoMA_img4.jpg"></div>
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">CHI 2018</header>
                        RoMA: Interactive Fabrication with Augmented Reality and a Robotic 3D Printer
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i>Huaishu Peng, <span class="bold">Cheng-Yao Wang</span>*, Jimmy Briggs*, Kevin Guo, Joseph Kider, Stefanie Mueller, Patrick Baudisch, François Guimbretière. (*equal contribution)</h4>
                    <h3 class="record"><i class="fa fa-book"></i>Published in CHI 2018 (full paper)</h3>             
                    <h3>We present the Robotic Modeling Assistant (RoMA), an interactive fabrication system providing a fast, precise, hands-on and in-situ modeling experience with an augmented reality CAD editor and a robotic arm 3D printer. With RoMA, users can integrate real-world constraints into a design rapidly, allowing them to create well-proportioned tangible artifacts. Users can even directly design on and around an existing object, and extending the artifact by in-situ fabrication.</h3>
                    <ul class="list-btn">
                        <!--<li><a href="#workModal3" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>-->
                        <li><a href="http://www.huaishu.me/projects/roma.pdf" target="_blank" class="btn-paper"><i class="fa fa-file-text"></i>paper</a></li>                       
                        <li><a href="https://www.youtube.com/watch?v=K_wWuYD1Fkg" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- RoMA project -->
            <!-- Reduct project-->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style small-padding">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/reduct_img1.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/reduct_img2.png"></div>
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">CHI 2017</header>
                        Teaching Programming with Gamified Semantics
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i>Ian Arawjo, <span class="bold">Cheng-Yao Wang,</span> Andrew C. Myers, Erik Andersen, and François Guimbretière</h4>
                    <h3 class="record"><i class="fa fa-book"></i>Published in CHI 2017 (full paper)</h3>             
                    <h3>We present Reduct, an educational game embodying a new, comprehension-first approach to teaching novices core programming concepts which include functions, Booleans, equality, conditionals, and mapping functions over sets. In this novel teaching strategy, the player executes code using reduction-based operational semantics. During gameplay, code representations fade from concrete, block-based graphics to the actual syntax of JavaScript ES2015. Our study result shows that novices demonstrated promising learning of core concepts expressed in actual JavaScript code in a short timeframe with our Reduct game.</h3>
                    <ul class="list-btn">
                        <!--<li><a href="#workModal3" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>-->
                        <li><a href="https://dl.acm.org/citation.cfm?id=3025711" target="_blank" class="btn-paper"><i class="fa fa-file-text"></i>paper</a></li>                       
                        <li><a href="https://www.youtube.com/watch?v=vSXBA95uQ04" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- Reduct project -->
            <!-- PalmType project-->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style small-padding">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/p-t-palm.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/p-t-img1.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/p-t-img3.jpg"></div>
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">MobileHCI 2015</header>
                        PalmType: Using Palms as Keyboards for Smart Glasses
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Wei-Chen Chu, Po-Tsung Chiu, Min-Chieh Hsiu, Yih-Harn Chiang, Mike Y. Chen</h4>
                    <h3 class="record"><i class="fa fa-book"></i>Published in MobileHCI 2015 (full paper)</h3>             
                    <h3>We present PalmType, which uses palms as interactive keyboards for smart wearable displays like Google Glass. PalmType leverages user's innate ability to pinpoint a specific area of palm and fingers without visual attention (i.e. proprioception) and provides visual feedback via wearable displays. With wrist-worn sensors and wearable displays, PalmType enables typing without requiring users to hold any devices and visual attention to their hands. We conducted design sessions with 6 participants to see how users map QWERTY layout to their hands based on their proprioception.</h3>
                    <ul class="list-btn">
                        <li><a href="#workModal3" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>
                        <li><a href="http://dl.acm.org/citation.cfm?id=2785886" target="_blank" class="btn-paper"><i class="fa fa-file-text"></i>paper</a></li>                       
                        <li><a href="https://www.youtube.com/watch?v=mwjADJ-v8nU" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- PalmType project -->
            <!-- PalmGesture project-->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/p-g-app1.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/p-g-img1.jpg"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/p-g-app3.png"></div>
                    </div>                    
                </div>                        
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">MobileHCI 2015</header>
                        PalmGesture: Using Palms as Gesture Interfaces for Eyes-free Input
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Min-Chieh Hsiu, Po-Tsung Chiu, Chiao-Hui Chang, Liwei Chan, Bing-Yu Chen, Mike Y. Chen</h4>                     
                    <h3 class="record"><i class="fa fa-book"></i>Published in MobileHCI 2015 (full paper)</h3>
                    <h3>With abundant tactile cues and proprioception on palms, the palm can be leveraged as an interface for eyes-free input which decreases visual attention to interfaces and minimizes cognitive/physical effort. We explored eyes-free gesture interactions on palms that enables users to interact with devices by drawing stroke gestures on palms without looking at palms. To understand user behavior when users draw gestures on palms and how gestures on palms are affected by palm characteristics, we conducted two 24-participant user studies. Also, we implemented EyeWrist that turns the palm into a gesture interface by embedding a micro-camera and an IR laser line generator on the wristband, and three interaction techniques that takes advantages of palm characteristics are proposed. </h3>
                    <ul class="list-btn">
                        <li><a href="#workModal5" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>
                        <li><a href="http://dl.acm.org/citation.cfm?id=2785885" target="_blank" class="btn-paper"><i class="fa fa-file-text"></i>paper</a></li>                        
                        <li><a href="https://www.youtube.com/watch?v=Ia7kr6-lRYw" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- PalmGesture project -->
            <!-- EverTutor project -->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-t-gestures.jpg"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-t-browse.jpg"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-t-create.jpg"></div>
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con1">CHI 2014</header>
                        EverTutor: Automatically Creating Interactive Guided Tutorials on Smartphones by User Demonstration
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Wei-Chen Chu, Hou-Ren Chen, Chun-Yen Hsu, Mike Y. Chen</h4>
                    <h3 class="record"><i class="fa fa-book"></i>Published in CHI 2014 (full paper)</h3>
                    <h3>We present EverTutor that automatically generates interactive tutorials on smartphone from user demonstration. It simplifies the tutorial creation, provides tutorial users with contextual step-by-step guidance and avoids the frequent context switching between tutorials and users' primary tasks. In order to generate tutorials automatically, EverTutor records low-level touch events to detect gestures and identify on-screen targets. When a tutorial is browsed, the system uses vision-based techniques to locate the target regions and overlays the corresponding input prompt contextually. It also identifies the correctness of users' interaction to guide the users step by step.</h3>
                    <ul class="list-btn">
                        <li><a href="#workModal2" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>
                        <li><a href="https://dl.acm.org/citation.cfm?id=2557407" target="_blank" class="btn-paper"><i class="fa fa-file-text"></i>paper</a></li>
                        <li><a href="https://www.youtube.com/watch?v=IubKeveE0Xg" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- EverTutor project -->
            <!-- RealSense project -->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/r-s-img1.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/r-s-logo.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/r-s-img2.png"></div>
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con2">MM 2013</header>
                        RealSense: Directional Interaction for Proximate Mobile Sharing
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i>Chien-Pang Lin, <span class="bold">Cheng-Yao Wang,</span>Hou-Ren Chen, Wei-Chen Chu, Mike Y. Chen</h4>
                    <h3 class="record"><i class="fa fa-book"></i>Published in MM 2013 (short paper)</h3>
                    <h3>We present RealSense, a technology that enables users to easily share media files with proximate users by detecting the relative direction of each other only with built-in orientation sensors on smartphones. With premise that users are arranged as a circle and every user is facing the center of that circle, RealSense continuously collects the directional heading of each phone to calculate the virtual position of each user in real time during the sharing.</h3>
                    <ul class="list-btn">
                        <li><a href="#workModal1" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>
                        <li><a href="https://dl.acm.org/citation.cfm?id=2502202" target="_blank" class="btn-paper"><i class="fa fa-file-text"></i>paper</a></li>
                        <li><a href="https://www.youtube.com/watch?v=p5gehFvSIE8" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- RealSense project --> 
            <!-- EyeWrist project -->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-w-prototype2.jpg"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-w-scenario1.jpg"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-w-prototype.jpg"></div>
                    </div>                                       
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>EyeWrist: Enabling Gesture-Based Interaction on Palm with a Wrist-Worn Sensor</h2>
                    <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Po-Tsung Chiu, Min-Chieh Hsiu, Chiao-Hui Chang, Mike Y. Chen</h4>
                    <h3><span class="span-focus"><i class="fa fa-trophy"></i>Excellent Work,</span>The 8th Acer Long-Term Smile Innovation Contest, 2014</h3>
                    <h3>We present EyeWrist, which uses palms as the gesture interface for smart wearable displays such as Google Glass. With abundant tactile cues and proprioception on palms, EyeWrist can also be leveraged for device-less and eyes-free remote for smart TVs. EyeWrist embeds a micro-camera and an IR laser line generator on the wristband and use computer vision algorithms to calculate the finger’s position on the palm. Without requiring the line of sight of users’ fingertips on palms, the camera height could be lower, making the whole device more portable. We also implemented a gesture recognizer to distinguish different symbols, letters or touchscreen gestures(e.g. swipe, pinch) on palms. The recognition result would be sent to smart devices via Wi-Fi for gesture-based interaction.</h3>
                    <ul class="list-btn">
                        <li><a href="#workModal4" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>
                        <li><a href="/" target="_blank" class="btn-paper"><i class="fa fa-file-text"></i>poster</a></li>
                        <li><a href="https://www.youtube.com/watch?v=WV7VwQw9OPM" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- EyeWrist project -->
            <!-- EyeWatch project -->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-wa-logo.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-wa-img1.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-wa-img2.png"></div>
                    </div>                    
                </div>
                <div class="col-md-7 text-left text-style">
                    <h2>EyeWatch: Touch Interaction on Back of the Hand for Smart Watches</h2>
                    <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Po-Tsung Chiu, Min-Chieh Hsiu</h4>
                    <h3><span class="span-focus"><i class="fa fa-trophy"></i>2nd Prize,</span>MobileHero – user experience design competition, 2014</h3>                      
                    <h3>We present EyeWatch, which uses back of the hand as gesture interface for smart watches. EyeWatch not only overcomes the Big smartwatch problem: occlusion and fat finger problem, but also enables more powerful and natural interaction such as drawing a symbol quickly to open an application, or intuitively handwriting on back of hand to input message. Our proof-of-concept implementation consists of a micro-camera and an IR laser line generator on the smart watch, and computer vision algorithms are used to calculate the finger’s position on the back of hand.</h3>
                    <ul class="list-btn">
                        <li><a href="#workModal6" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>                        
                        <li><a href="https://www.youtube.com/watch?v=j-BhWbpgHZk" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- EyeWatch project -->
            <!-- Shrinking project -->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style">                    
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/s-a-screen.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/s-a-sketch.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/s-a-car2.jpg"></div>
                    </div>                                       
                </div>
                <div class="col-md-7 text-left text-style">
                    <h2>The Incredible Shrinking Adventure</h2>
                    <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Min-Chieh Hsiu, Chin-Yu Chien, Shuo Yang</h4>
                    <h3><span class="span-focus"><i class="fa fa-trophy"></i>Final Shortlist,</span>ACM UIST 2014 Student Innovation Contest</h3>
                    <h3>Imagining you were shrunk, you can explore your big house, play with big pets and family. To fulfill the imagination and provide users with incredible shrinking adventures, we use a robotic car and a google cardboard which turns a smartphone into a VR headset. We build a robotic car and attach a smartphone on the pan/tilt servo bracket. stereo images are generated from smartphone’s camera and are streamed to the other smartphone inside of the google cardboard. When users see the world through the smartphone on robotic car, they feel they were shrunk.</h3>
                    <ul class="list-btn">
                        <li><a href="#workModal7" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>
                        <li><a href="img/projects/poster-lilliput_small.png" class="btn-paper" data-toggle="lightbox"><i class="fa fa-file-text"></i>poster</a></li>
                        <li><a href="https://www.youtube.com/watch?v=UPvq2fLKTsQ" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- Shrinking project -->
            <!-- droneIO project -->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style">                    
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/droneio_1.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/droneio_2.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/droneio_3.png"></div>
                    </div>                                       
                </div>
                <div class="col-md-7 text-left text-style">
                    <h2>drone.io: Gesture Input and Projected Output for Collocated Human-Drone Interaction</h2>
                    <!--<h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Min-Chieh Hsiu, Chin-Yu Chien, Shuo Yang</h4>-->
                    <h3 class="record"><i class="fa fa-book"></i>Reserach intern project at Stanford University (2015) </h3>
                    <h3>We introduce drone.io, a body-centric interface to facilitate natural HDI where users interact with simple gestures above a projected radial menu. drone.io is a fully embedded input-output system using a depth camera to recognize the position and shape of the user’s hand and a mobile projector to display the interface in real time. We show that it is an easy, effective, and enjoyable way for people to interact with drones.</h3>
                    <!--
                    <ul class="list-btn">
                        <li><a href="#workModal7" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>
                        <li><a href="img/projects/poster-lilliput_small.png" class="btn-paper" data-toggle="lightbox"><i class="fa fa-file-text"></i>poster</a></li>
                        <li><a href="https://www.youtube.com/watch?v=UPvq2fLKTsQ" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul> -->                   
                </div>
            </div>
            <!-- droneIO project -->                                   
        </div>
    </section>

    <!-- skill section -->
    <section id="skills">
        <div class="container">
            <h1>my skills</h1>
            <div class="row">
                <div class="col-md-4 skill-text">
                    <h2>Computer Science</h2>
                    <ul>
                        <li>C#, Unity, Web Languages(JavaScript/CSS/NodeJS), C/C++, JAVA, Python</li>
                        <li>Human-Computer Interaction, Virtual Reality, Augmented Reality, Computer Graphic, Computer Vision</li>
                    </ul>
                </div>
                <div class="col-md-4 skill-text">
                    <h2>Rapid Prototyping</h2>
                    <ul>
                        <li>Design and Rapid Prototyping with laser cutter and 3D printing</li>
                        <li>Arduino, Raspberry Pi, Processing</li>
                    </ul>
                </div>
                <div class="col-md-4 skill-text">
                    <h2>User Study Design</h2>
                    <ul>
                        <li>Mixed qualitative and quantitative study design</li>
                        <li>Statistical analysis</li>
                    </ul>
                </div>
            </div>
        </div>
    </section>

    <!-- Publication Section -->
    <section id="publication">
        <div class="container">
            <div class="row">
                <div class="col-lg-5 text-center col-left-style">
                    <h1>publications</h1>
                    <div class="list-wrap text-left">
                        <h2><a href="http://www.huaishu.me/projects/roma.pdf" target="_blank">RoMA: Interactive Fabrication with Augmented Reality and a Robotic 3D Printer</a></h2>
                        <h3>ACM CHI 2018, Full Paper</h3>
                         <h4><i class="fa fa-users"></i>Huaishu Peng, <span class="bold">Cheng-Yao Wang</span>*, Jimmy Briggs*, Kevin Guo, Joseph Kider, Stefanie Mueller, Patrick Baudisch, François Guimbretière. (*equal contribution)</h4>              
                    </div>
                    <div class="list-wrap text-left">
                        <h2><a href="https://dl.acm.org/citation.cfm?id=3025711" target="_blank">Teaching Programming with Gamified Semantics</a></h2>
                        <h3>ACM CHI 2017, Full Paper</h3>
                         <h4><i class="fa fa-users"></i>Ian Arawjo, <span class="bold">Cheng-Yao Wang</span>, Andrew C. Myers, Erik Andersen, and François Guimbretière</h4>              
                    </div>
                    <div class="list-wrap text-left">
                        <h2><a href="http://dl.acm.org/citation.cfm?id=2785886" target="_blank">PalmType: Using Palms as Keyboards for Smart Glasses</a></h2>
                        <h3>ACM MobileHCI 2015, Full Paper</h3>
                         <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Wei-Chen Chu, Po-Tsung Chiu, Min-Chieh Hsiu, Yih-Harn Chiang, Mike Y. Chen</h4>              
                    </div>
                    <div class="list-wrap text-left">
                        <h2><a href="http://dl.acm.org/citation.cfm?id=2785885" target="_blank">PalmGesture: Using Palms as Gesture Interfaces for Eyes-free Input</a></h2>
                        <h3>ACM MobileHCI 2015, Full Paper</h3>
                        <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Min-Chieh Hsiu, Po-Tsung Chiu, Chiao-Hui Chang, Liwei Chan, Bing-Yu Chen, Mike Y. Chen</h4>              
                    </div> 
                    <div class="list-wrap text-left">
                        <h2><a href="https://dl.acm.org/citation.cfm?id=2557407" target="_blank">EverTutor: Automatically Creating Interactive Guided Tutorials on Smartphones by User Demonstration</a></h2>
                        <h3>ACM CHI 2014, Full Paper</h3>
                        <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Wei-Chen Chu, Hou-Ren Chen, Chun-Yen Hsu, Mike Y. Chen</h4>
                    </div>
                    <div class="list-wrap text-left">
                        <h2><a href="https://dl.acm.org/citation.cfm?id=2502202" target="_blank">RealSense: Directional Interaction for Proximate Mobile Sharing Using Built-in Orientation Sensors</a></h2>
                        <h3>MM 2013, short paper</h3>
                        <h4><i class="fa fa-users"></i>Chien-Pang Lin, <span class="bold">Cheng-Yao Wang,</span>Hou-Ren Chen, Wei-Chen Chu, Mike Y. Chen</h4>
                    </div>
                    <div class="list-wrap text-left">
                        <h2>drone.io: Gesture Input and Projected Output for Collocated Human-Drone Interaction</h2>
                        <h3>Under review for ACM CHI 2016</h3>
                    </div>                    
                </div>
                <div class="col-lg-7 text-center col-right-style">
                    <h1>awards</h1>
                    <div class="list-wrap text-left">
                        <h2>EverTutor: Automatically Creating Interactive Guided Tutorials on Smartphones by User Demonstration</h2>
                        <h3><span class="span-focus"><i class="fa fa-trophy"></i>1st Prize,</span>The 11th Deep Innovations with Impact, National Taiwan University, 2013</h3>
                        <h3><span class="span-focus"><i class="fa fa-trophy"></i>1st Prize,</span>The 11th Y.S. National Innovation Software Application Contest, 2014</h3>  
                    </div>
                    <div class="list-wrap text-left">
                        <h2>MagicWrist - Connect the world</h2>
                        <h3><span class="span-focus"><i class="fa fa-trophy"></i>Excellent Work,</span>The 8th Acer Long-Term Smile Innovation Contest, 2014</h3>            
                    </div>
                    <div class="list-wrap text-left">
                        <h2>The Incredible Shrinking Adventure</h2>
                        <h3><span class="span-focus"><i class="fa fa-trophy"></i>Final Shortlist,</span>ACM UIST 2014 Student Innovation Contest</h3>                      
                    </div>
                    <div class="list-wrap text-left">
                        <h2>EyeWatch: Touch Interaction on Back of the Hand for Smart Watches</h2>
                        <h3><span class="span-focus"><i class="fa fa-trophy"></i>2nd Prize,</span>MobileHero – user experience design competition, 2014</h3>               
                    </div>
                    <div class="list-wrap text-left">
                        <h2>EyeWrist - Enabling Gesture-Based Interaction on Palm with a Wrist-Worn Sensor</h2>
                        <h3><span class="span-focus"><i class="fa fa-trophy"></i>Final Shortlist,</span>MediaTek Wearable device into IoT world competition, 2014</h3>               
                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer>
        <div class="container">
            <div class="row">
                <div class="col-md-4">
                    <h3>Copyright &copy; 2014 Eric Wang</h3>
                </div>
                <div class="col-md-4">
                    <ul class="icon-list">
                        <li><a href="mailto:ericwang0701@gmail.com"><i class="fa fa-envelope-square fa-3x"></i></a></li>
                        <li><a href="https://www.facebook.com/profile.php?id=100000233739748" target="_blank"><i class="fa fa-facebook-square fa-3x"></i></a></li>
                        <li><a href="https://www.youtube.com/channel/UCbBZl_n5DbM4i8xhXJrdRIg" target="_blank"><i class="fa fa-youtube-square fa-3x"></i></a></li>
                        <li><a href="https://www.linkedin.com/in/ericwanghci" target="_blank"><i class="fa fa-linkedin-square fa-3x"></i></a></li>
                    </ul>
                </div>
                <div class="col-md-4">
                    <h3>Last Update: Jan. 2015</h3>
                </div>
            </div>
        </div>
    </footer>

    <!-- Project Modals -->    

    <!-- Project Modal 1 -->
    
    <div class="work-modal modal fade" id="workModal1" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="modal-body">                    
                    <div class="row">
                        <div class="col-sm-8 col-sm-offset-2 text-left">
                            <div class="modal-title text-center">
                                <h2>RealSense:<br>Directional Interaction for Proximate Mobile Sharing</h2>
                            </div>                                                                                   
                            <div class="item active embed-responsive embed-responsive-16by9 video-wraper">                                
                                <iframe src="https://www.youtube.com/embed/p5gehFvSIE8" frameborder="0" allowfullscreen></iframe>                                
                            </div>                                                                                                  
                            <h4><i class="fa fa-users"></i>Chien-Pang Lin, <span class="bold">Cheng-Yao Wang,</span>Hou-Ren Chen, Wei-Chen Chu, Mike Y. Chen</h4>
                            <h3 class="text-center">Published in <span class="span-focus">MM 2013</span>(short paper)</h3>
                            <h3>We present RealSense, a technology that enables users to easily share media files with proximate users by detecting the relative direction of each other only with built-in orientation sensors on smartphones. With premise that users are arranged as a circle and every user is facing the center of that circle, RealSense continuously collects the directional heading of each phone to calculate the virtual position of each user in real time during the sharing.</h3>
                            <h3>RealSense users can simply share photos to others by swiping and throwing the photo to another user’s direction without remembering or searching the name and even the device id of a specific receiver. We evaluated the orientation sensor error and the minimal arc degree for selection, and compared RealSense with linear menu, pie menu and NFC. Our results show that the limitation that requires participants to face toward the circle center is rather acceptable and participants preferred RealSense than other sharing interactions especially they were unacquainted with each other.</h3>                                                                                                             
                        </div>
                    </div>
                    <div class="img-wraper text-center">
                        <img class="img-responsive img-centered" src="img/projects/r-s-img3.jpg">
                        <img class="img-responsive img-centered" src="img/projects/r-s-img4.jpg">
                    </div>
                    <button type="button" class="btn btn-primary" data-dismiss="modal"><i class="fa fa-times"></i> Close Project</button>
                </div>
            </div>
        </div>
    </div>

    <!-- Project Modal 2 -->

    <div class="work-modal modal fade" id="workModal2" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="modal-body">                    
                    <div class="row">
                        <div class="col-sm-8 col-sm-offset-2 text-left">
                            <div class="modal-title text-center">
                                <h2>EverTutor:<br>Automatically Creating Interactive Guided Tutorials on Smartphones by User Demonstration</h2>
                            </div>
                            <div class="item active embed-responsive embed-responsive-16by9 video-wraper">
                                <iframe src="https://www.youtube.com/embed/IubKeveE0Xg" frameborder="0" allowfullscreen></iframe>                                
                            </div>                                                                                                  
                            <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Wei-Chen Chu, Hou-Ren Chen, Chun-Yen Hsu, Mike Y. Chen</h4>
                            <h3>Published in <span class="span-focus">CHI 2014</span>(full paper)</h3>
                            <h3>We present EverTutor, a system that automatically generates interactive tutorials on smartphone from user demonstration. For tutorial authors, it simplifies the tutorial creation. For tutorial users, it provides contextual step-by-step guidance and avoids the frequent context switching between tutorials and users' primary tasks. In order to generate the tutorials automatically, EverTutor records low-level touch events to detect gestures and identify on-screen targets. When a tutorial is browsed, the system uses vision-based techniques to locate the target regions and overlays the corresponding input prompt contextually. It also identifies the correctness of users' interaction to guide the users step by step.</h3>
                            <h3>We conducted a 6-person user study for creating tutorials and a 12-person user study for browsing tutorials, and we compared EverTutor's interactive tutorials to static and video ones. Study results show that creating tutorials by EverTutor is simpler and faster than producing static and video tutorials. Also, when using the tutorials, the task completion time for interactive tutorials were 3-6 times faster than static and video tutorials regardless of age group. In terms of user preference, 83% of the users chose interactive type as the preferred tutorial type and rated it easiest to follow and easiest to understand.</h3>                                                       
                        </div>
                    </div>
                    <button type="button" class="btn btn-primary" data-dismiss="modal"><i class="fa fa-times"></i> Close Project</button>
                </div>
            </div>
        </div>
    </div>

    <!-- Project Modal 3 -->

    <div class="work-modal modal fade" id="workModal3" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="modal-body">                      
                    <div class="row">
                        <div class="col-sm-8 col-sm-offset-2 text-left">
                            <div class="modal-title text-center">
                                <h2>PalmType: <br>Using Palms as Keyboards for Smart Glasses</h2>
                            </div>
                            <div class="item active embed-responsive embed-responsive-16by9 video-wraper">
                                <iframe src="https://www.youtube.com/embed/mwjADJ-v8nU" frameborder="0" allowfullscreen></iframe>                                
                            </div>                                                                                                  
                            <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Wei-Chen Chu, Po-Tsung Chiu, Yih Harn Chiang, Mike Y. Chen</h4>  
                            <h3>We present PalmType, which uses palms as interactive key- boards for smart wearable displays, such as Google Glass. PalmType leverages users’ innate ability to pinpoint specific areas of their palms and fingers without visual attention (i.e. proprioception), and provides visual feedback via the wear- able displays. With wrist-worn sensors and wearable dis- plays, PalmType enables typing without requiring users to hold any devices and does not require visual attention to their hands. We conducted design sessions with 6 participants to see how users map QWERTY layout to their hands based on their proprioception.</h3>
                            <h3>To evaluate typing performance and preference, we conducted a 12-person user study using Google Glass and Vicon motion tracking system, which showed that PalmType with optimized QWERTY layout is 39% faster than current touchpad-based keyboards. In addition, PalmType is preferred by 92% of the participants. We demonstrate the feasibility of wearable PalmType by building a prototype that uses a wrist-worn array of 15 infrared sensors to detect users’ finger position and taps, and provides visual feedback via Google Glass.</h3>                                                       
                        </div>
                    </div>
                    <button type="button" class="btn btn-primary" data-dismiss="modal"><i class="fa fa-times"></i> Close Project</button>
                </div>
            </div>
        </div>
    </div>

    <!-- Project Modal 4 -->

    <div class="work-modal modal fade" id="workModal4" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="modal-body">                    
                    <div class="row">
                        <div class="col-sm-8 col-sm-offset-2 text-left">
                            <div class="modal-title text-center">
                                <h2>EyeWrist: <br>Enabling Gesture-Based Interaction on Palm with a Wrist-Worn Sensor</h2>
                            </div>
                            <div class="item active embed-responsive embed-responsive-16by9 video-wraper">
                                <iframe src="https://www.youtube.com/embed/WV7VwQw9OPM" frameborder="0" allowfullscreen></iframe>                                
                            </div>                                                                                                  
                            <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Po-Tsung Chiu, Min-Chieh Hsiu, Chiao-Hui Chang, Mike Y. Chen</h4>
                            <h3><span class="span-focus"><i class="fa fa-trophy"></i>Excellent Work,</span>The 8th Acer Long-Term Smile Innovation Contest, 2014</h3>   
                            <h3>We present EyeWrist, which uses palms as the gesture interface for smart wearable displays such as Google Glass. With abundant tactile cues and proprioception on palms, EyeWrist can also be leveraged for device-less and eyes-free remote for smart TVs. EyeWrist embeds a micro-camera and an IR laser line generator on the wristband and use computer vision algorithms to calculate the finger’s position on the palm. Without requiring the line of sight of users’ fingertips on palms, the camera height could be lower, making the whole device more portable. We also implemented a gesture recognizer to distinguish different symbols, letters or touchscreen gestures(e.g. swipe, pinch) on palms. The recognition result would be sent to smart devices via Wi-Fi for gesture-based interaction.</h3>                                                       
                        </div>
                    </div>
                    <button type="button" class="btn btn-primary" data-dismiss="modal"><i class="fa fa-times"></i> Close Project</button>
                </div>
            </div>
        </div>
    </div>

    <!-- Project Modal 5 -->

    <div class="work-modal modal fade" id="workModal5" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="modal-body">                                         
                    <div class="row">
                        <div class="col-sm-8 col-sm-offset-2 text-left">
                            <div class="modal-title text-center">
                                <h2>PalmGesture: <br>Using Palms as Gesture Interfaces for Eyes-free Input</h2>
                            </div>
                            <div class="item active embed-responsive embed-responsive-16by9 video-wraper">
                                <iframe src="https://www.youtube.com/embed/Ia7kr6-lRYw" frameborder="0" allowfullscreen></iframe>                                
                            </div>                                                                                                  
                            <h3>Submitted to ACM MobileHCI 2015, Full Paper</h3> 
                            <h3>With abundant tactile cues and proprioception on palms, the palm can be leveraged as an interface for eyes-free input which decreases visual attention to interfaces and minimizes cognitive/physical effort. In this paper, we explored eyes-free gesture interactions on palms, which enables users to interact with devices by drawing stroke gestures on palms without looking at palms. In two 24-person user studies, we observed that (1) users preferred using the whole palm region as the gesture interface with 3 categories of hand orientations. (2) users tended to draw different gestures from the same start region due to proprioception on palms. (3) the accuracy of eyes-free Graffiti input and multi-stroke gesture recognizer on palm interface reached as high as 98% and 95% respectively. (4) the tactile feedback of palms can significantly help participants build up spatial memory of gestures. Also, we implemented EyeWrist that turns the palm into a gesture interface by em- bedding a micro-camera and an IR laser line generator on the wristband, and three interaction techniques that takes advantages of palm characteristics are proposed. The preliminary evaluation revealed that EyeWrist enabled users to draw graffiti letter and multi-stroke gestures with 90% above accuracy, and that the concept of eyes-free palm-based gesture interaction was appealing to users.</h3>                                                       
                        </div>
                    </div>
                    <button type="button" class="btn btn-primary" data-dismiss="modal"><i class="fa fa-times"></i> Close Project</button>
                </div>
            </div>
        </div>
    </div>

    <!-- Project Modal 6 -->

    <div class="work-modal modal fade" id="workModal6" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="modal-body">                                        
                    <div class="row">
                        <div class="col-sm-8 col-sm-offset-2 text-left">
                            <div class="modal-title text-center">
                                <h2>EyeWatch: <br>Touch Interaction on Back of the Hand for Smart Watches</h2>
                            </div>
                            <div class="item active embed-responsive embed-responsive-16by9 video-wraper">
                                <iframe src="https://www.youtube.com/embed/j-BhWbpgHZk" frameborder="0" allowfullscreen></iframe>                                
                            </div>                                                                                                  
                            <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Po-Tsung Chiu, Min-Chieh Hsiu</h4>
                            <h3><span class="span-focus">12/03 Final,</span>MobileHero – user experience design competition, 2014</h3> 
                            <h3>We present EyeWatch, which uses back of the hand as gesture interface for smart watches. EyeWatch not only overcomes the Big smartwatch problem: occlusion and fat finger problem, but also enables more powerful and natural interaction such as drawing a symbol quickly to open an application, or intuitively handwriting on back of hand to input message. Our proof-of-concept implementation consists of a micro-camera and an IR laser line generator on the smart watch, and computer vision algorithms are used to calculate the finger’s position on the back of hand.</h3>                                                       
                        </div>
                    </div>
                    <button type="button" class="btn btn-primary" data-dismiss="modal"><i class="fa fa-times"></i> Close Project</button>
                </div>
            </div>
        </div>
    </div>

    <!-- Project Modal 7 -->

    <div class="work-modal modal fade" id="workModal7" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="modal-body">                                      
                    <div class="row">
                        <div class="col-sm-8 col-sm-offset-2 text-left">
                            <div class="modal-title text-center">
                                <h2>The Incredible Shrinking Adventure</h2>
                            </div>
                            <div class="item active embed-responsive embed-responsive-16by9 video-wraper">
                                <iframe src="https://www.youtube.com/embed/UPvq2fLKTsQ" frameborder="0" allowfullscreen></iframe>                                
                            </div>                                                                                                  
                            <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Min-Chieh Hsiu, Chin-Yu Chien, Shuo Yang</h4>
                            <h3>ACM UIST 2014 Student Innovation Contest</h3> 
                            <h3>Imagining you were shrunk, you can explore your big house, play with big pets and family. To fulfill the imagination and provide users with incredible shrinking adventures, we use a robotic car and a google cardboard which turns a smartphone into a VR headset. We build a robotic car and attach a smartphone on the pan/tilt servo bracket. stereo images are generated from smartphone’s camera and are streamed to the other smartphone inside of the google cardboard. When users see the world through the smartphone on robotic car, they feel they were shrunk.</h3>                                                       
                        </div>
                    </div>
                    <button type="button" class="btn btn-primary" data-dismiss="modal"><i class="fa fa-times"></i> Close Project</button>
                </div>
            </div>
        </div>
    </div>

       

    <!-- jQuery Version 1.11.0 -->
    <script src="js/jquery-1.11.0.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="js/agency.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
    <script src="js/classie.js"></script>
    <script src="js/cbpAnimatedHeader.js"></script>

    <!-- Custom Theme JavaScript -->    
    <script src="js/ekko-lightbox.min.js"></script>
    <script src="js/jquery.lazyload.min.js"></script>
    <script src="slick/slick.min.js"></script>


    <script type="text/javascript">
        
        $(function() {
            
            $(document).delegate('*[data-toggle="lightbox"]', 'click', function(event) {
                event.preventDefault();
                $(this).ekkoLightbox();
            }); 
            
            $('#myHeader').lazyload({
                effect : "fadeIn"
            });

            $('img.lazy').lazyload({
                effect : "fadeIn"
            });

            $('.my-slide').slick({
              dots: true,
              infinite: true,
              speed: 500,
              fade: true,
              arrows: false,              
              cssEase: 'linear',
              lazyLoad: 'progressive'
            });            

            //stop you tube player when bootstrap modal was hiden
            $('.modal').on('hide.bs.modal', function () {
               $(this).find('iframe').attr("src", $(this).find('iframe').attr("src"));
            })    
        });

    </script>


</body>

</html>
