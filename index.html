<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Eric Wang">

    <title>Eric Wang</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/ekko-lightbox.min.css" rel="stylesheet">
    <link href="slick/slick.css" rel="stylesheet">
    <link href="css/onepage.css" rel="stylesheet">
    

    <!-- Custom Fonts -->
    <link href="font-awesome-4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300' rel='stylesheet' type='text/css'>    

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" class="index">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">Eric Wang</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#about">about</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#project">projects</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#skills">skills</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#publication">publications</a>
                    </li>                                        
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

    <!-- Header -->
    <header id="myHeader" data-original="img/top-profile3.jpg">
        <div class="container">
            <div class="intro-text">
                <!--<div id="profile-wraper"></div>-->
                <div class="text-wraper">
                    <div class="intro-heading">Eric Wang<p style="font-size:0.5em;">(Cheng-Yao Wang)</p></div>
                    <div class="intro-lead-in">
                        <p>I’m Eric Wang, a Ph.D. student in Information Science at Cornell University interested in <span class="about-focus">Human-Computer Interaction</span>, <span class="about-focus">Computer-Supported Cooperative Work</span> and <span class="about-focus">Virtual/Mixed Reality.</span></p>
                    </div>
                    <ul class="icon-list">
                        <li><a href="mailto:ericwang0701@gmail.com"><i class="fa fa-envelope-square fa-3x"></i></a></li>
                        <li><a href="https://www.facebook.com/profile.php?id=100000233739748" target="_blank"><i class="fa fa-facebook-square fa-3x"></i></a></li>
                        <li><a href="https://www.youtube.com/channel/UCbBZl_n5DbM4i8xhXJrdRIg" target="_blank"><i class="fa fa-youtube-square fa-3x"></i></a></li>
                        <li><a href="https://www.linkedin.com/in/ericwanghci" target="_blank"><i class="fa fa-linkedin-square fa-3x"></i></a></li>
                    </ul>
                </div>
            </div>
        </div>
    </header>

    <!-- About Section -->
    <section id="about">
        <div class="container">
            <h1>About me</h1>
            <div class="row text-left about-header">
                <div class="col-md-3 about-thumb">
                    <div id="profile-wraper"></div>
                </div>
                <div class="col-md-9 about-content">
                    <h3>I’m Eric Wang, a Ph.D. student in Information Science at Cornell University advised by <a href="https://communication.cals.cornell.edu/people/andrea-won" target="_blank">Prof. Andrea Stevenson Won</a>. Previously, I was a research intern at Stanford University, primarily working with <a href="https://profiles.stanford.edu/james-landay/" target="_blank">Prof. James Landay</a>. I have B.S and M.S. in Computer Science from National Taiwan University advised by <a href="http://mikechen.com/" target="_blank">Prof. Mike Y. Chen</a> and <a href="http://graphics.im.ntu.edu.tw/~robin/" target="_blank">Prof. Bing-Yu Chen</a>.</h3>

                    <h3>My research interests lie at the intersection of <span class="span-focus">Human-Computer Interaction (HCI)</span>, <span class="span-focus">Computer-Supported Cooperative Work (CSCW) </span>, and <span class="span-focus">Virtual/Mixed Reality (VR/MR)</span> My PhD research topic is <span class="bold">enabling VR socially relived experience</span> which allows people to ‘enter’ into a past experience, move around and relive a experience from different perspectives in VR socially and exploring <span class="bold">how such new VR relived experiences transform the way people share experiences</span>.</h3>

                    <h3>More specifically, I’m interested in (1) utilizing deep learning, computer vision and VR to enable reconstructing past experience in 3D and sharing experiences through socially reliving experiences in VR; (2)  investigating how people identify with their reconstructed avatars and privacy concerns when sharing VR relived experience, and(3) designing novel mechanisms allowing people to customize their reconstructed experiences for self-presentation while preserving the privacy. </h3>
    
                </div>                
            </div>
            <div class="row text-left font-height">
                <div class="col-md-3">
                    <h2>Education</h2>
                </div>
                <div class="col-md-9 about-list">
                    <ul>
                        <li class="list-left"><h3><span class="bold">Ph.D.</span> in Information Science, Cornell University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Sep '16 – Current</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">M.S.</span> in Computer Science, National Taiwan University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Sep '12 – Jun '14</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">B.S.</span> in Computer Science, National Taiwan University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Feb '06 – Aug '10</h5></li>
                    </ul>                    
                </div>                               
            </div>
            <div class="row text-left font-height">
                <div class="col-md-3">
                    <h2>Work Experience</h2>
                </div>
                <div class="col-md-9 about-list">
                    <ul>
                        <li class="list-left"><h3><span class="bold">Research Internship,</span>work with Dr. Mar Gonzalez Franco and Dr. Andy Wilson, Microsoft Research</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Summer 2022</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Workshop Co-organizer,</span>Open Access Tools and Libraries for Virtual Reality, IEEE VR 2022</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Spring 2022</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Research Internship,</span>work with Dr. Mark Parent and Dr. Marcello Giordano, Facebook Reality Lab</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Fall 2021 & Winter 2022</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Research Internship,</span>work with Dr. Mar Gonzalez Franco and Dr. Eyal Ofek, Microsoft Research</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Summer 2021</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Research Internship,</span>work with Dr. Fraser Anderson and Dr. Qian Zhou, Autodesk HCI&VIS groop</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Winter 2021</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Research Assistant,</span>VR as a Teaching Tool for Moon Phases and Beyond, Cornell University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Fall 2019</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Research Assistant,</span>Remote Touch for Telepresence Robots, Cornell University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Spring 2018</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Research Intern,</span>Human-Drone Interaction project, Stanford University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Apr '15 – Sep '15</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Chief Coordinator,</span><a href="https://drive.google.com/file/d/0B-TOzIcx7m_melhIZkRpWGtHd0NJOGNnQi15VVdod2kzbGZz/view?usp=sharing" class="link" target="_blank">2015 CHI@NTU Workshop</a></h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Aug '14 – Sep '14</h5></li>
                    </ul>               
                </div>                                            
            </div>
            <div class="row text-left font-height">
                <div class="col-md-3">
                    <h2>Teaching Experience</h2>
                </div>
                <div class="col-md-9 about-list">
                    <ul>
                        <li class="list-left"><h3><span class="bold">Teaching Assistant,</span>Computer-mediated Communication, Cornell University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Spring 2022</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Teaching Assistant,</span>Introduction to Computing using Matlab, Cornell University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Fall 2018</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Teaching Assistant,</span>Crowdsourcing and Human Computation, Cornell University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Fall 2017</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Teaching Assistant,</span>Introduction to Computing using Matlab, Cornell University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Spring 2017</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Teaching Assistant,</span>Data Structures and Functional Programming, Cornell University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Fall 2016</h5></li>
                    </ul>
                    <ul>
                        <li class="list-left"><h3><span class="bold">Teaching Assistant,</span><a href="https://mobilehci.hackpad.com/2014S-Mobile-HCI-Class-Schedule-wM4Izj9r0Oa" class="link" target="_blank">Mobile HCI</a>, National Taiwan University</h3></li>
                        <li class="list-right"><h5><i class="fa fa-calendar"></i>Feb '14 – Jun '14</h5></li>
                    </ul> 
                </div>
            </div>
            <p class="btn-download text-center"><a href="https://drive.google.com/open?id=1FbAw15vjUw3MStqGBT2c85AJeXKQa3iK" class="link" target="_blank"><i class="fa fa-download"></i>download cv</a></p>
        </div>
    </section>

    <section id="skill-bar">
        <div class="my-skill">            
            <ul>
                <li>
                    <h2>Computer Vision</h2>
                    <p class="skill-img"><img class="lazy" data-original="img/skill1_2.png"></p>
                </li>
                <i class="fa fa-plus"></i>
                <li>
                    <h2>Virtual Reailty</h2>
                    <p class="skill-img"><img class="lazy" data-original="img/skill2_2.png"></p>
                </li>
                <i class="fa fa-plus"></i>
                <li>
                    <h2>Mixed method Research</h2>
                    <p class="skill-img"><img class="lazy" data-original="img/skill3.png"></p>
                </li>
            </ul>            
            <!-- <div class="h-wraper"><h3>Novel Technologies Published in Top Conference and 1st Prize of Competitions</h3></div> -->
            <div class="h-wraper"><h3>Transforming how people share knowledge and experiences</h3></div>
        </div>
    </section>

    <!-- project Section -->
    <section id="project">
        <div class="container">
            <h1>projects</h1>
            <!-- VideoPoseVR project-->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style small-padding">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/VideoPoseVR_teaser.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/VideoPoseVR_features.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/VideoPoseVR_dataset_all.png"></div>
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">Under Review</header>VideoPoseVR: Prototyping Animated Characters in VR through Reconstructing 3D Human Motion from Online Videos
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i><span class="bold">Cheng Yao Wang</span> et al.</h4>             
                    <h3 class="record"><i class="fa fa-book"></i>Under Review (full paper)</h3>
                    <h3>We introduce VideoPoseVR, a system that allows users to animate VR characters using human motions extracted from online videos. We demonstrate an end-to-end workflow that allows users to retrieve desired motion from 2D videos, edit the motion, and apply it to virtual characters in VR. \VPVR leverages deep learning-based computer vision techniques to reconstruct 3D human motions from videos and enable semantic searching of specific motions without annotating videos. Users can edit, mask, and blend motions from different videos to refine the movement. We implemented and evaluated a proof-of-concept prototype to demonstrate VideoPoseVR's interaction possibilities and use cases. The study results suggest that the prototype was easy to learn and use and that it could be used to quickly prototype immersive environments for applications such as entertainment, skills training, and crowd simulations.</h3>
                    <ul class="list-btn">
                        <li><a target="_blank" class="btn-paper"><i class="fa fa-caret-square-o-right"></i>Paper</a></li>
                        <li><a href="https://youtu.be/JEzur6TME2I" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>Video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- VideoPoseVR project -->
            <!-- Shared Realities project-->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style small-padding">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/SR_fig1.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/SR_env_hub_h_v2.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/SR_sharing_preference_results_v2.png"></div>
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">CSCW 2021</header>
                        Shared Realities: Avatar Identification and Privacy Concerns in Reconstructed Experiences
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i><span class="bold">Cheng Yao Wang</span>, Sandhya Sriram, and Andrea Stevenson Won.</h4>             
                    <h3 class="record"><i class="fa fa-book"></i>Published in ACM CSCW 2021 (full paper)</h3>
                    <h3>We present ReliveReality, an experience-sharing method utilizing deep learning-based computer vision techniques to reconstruct clothed humans and 3D environments and estimate 3D pose with only a RGB camera. ReliveReality can be integrated into social virtual environments, allowing others to socially relive a shared experience by moving around the experience from different perspectives, on desktop or in VR. We conducted a 44-participant within-subject study to compare ReliveReality to viewing recorded videos, and to a ReliveReality version with blurring obfuscation. Our results shed light on how people identify with reconstructed avatars, how obfuscation affects reliving experiences, and sharing preferences and privacy concerns for reconstructed experiences. We propose design implications for addressing these issues</h3>
                    <ul class="list-btn">
                        <li><a href="https://dl.acm.org/doi/abs/10.1145/3476078" target="_blank" class="btn-paper"><i class="fa fa-caret-square-o-right"></i>Paper</a></li>
                        <li><a href="https://www.youtube.com/watch?v=01pcAknvChQ" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>Video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- Shared Realities project -->
            <!-- Virtual Background project-->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style small-padding">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/vb_website_demo.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/vb_background_effect.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/vb_mturk_template.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/vb_jn_m1a.png"></div>
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">CSCW 2021</header> Hide and Seek: Choices of Virtual Backgrounds in Video Chats and Their Effects on Perception
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i><span class="bold">Cheng Yao Wang</span>*, Angel Hsing-Chi Hwang*, Yao-Yuan Yang, and Andrea Stevenson Won. (*equal contribution)</h4>             
                    <h3 class="record"><i class="fa fa-book"></i>Published in ACM CSCW 2021 (full paper)</h3>
                    <h3>We investigate how users choose virtual backgrounds and how these backgrounds influence viewers' impressions. In Study 1, we created a web prototype allowing users to apply different virtual backgrounds to their camera views and asked users to select backgrounds that they believed would change viewers' perceptions of their personality traits. In Study 2, we then applied virtual backgrounds picked by participants in Study 1 to a subset of videos drawn from the First Impression Dataset. Our study results suggested that the selected virtual backgrounds did not change the personality trait ratings in the intended direction. Instead, virtual background use of any kind results in a consistent "muting effect" that mitigates very high or low ratings (i.e., compressing ratings to the mean level) compared to the ratings of the video with the original background.</h3>
                    <ul class="list-btn">
                        <li><a href="https://dl.acm.org/doi/abs/10.1145/3476044" target="_blank" class="btn-paper"><i class="fa fa-caret-square-o-right"></i>Paper</a></li>
                        <li><a href="https://www.youtube.com/watch?v=3Px-UQCbJ5s" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>Video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- Virtual Background project -->
            <!-- RelivingVRV2 project-->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style small-padding">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/ReliveInVR_study.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/Co-WatchDT.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/Co-watchVR.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/ReliveInVR.png"></div>                     
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">CHI 2020</header>Again, Together: Socially Reliving Virtual Reality Experiences When Separated
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang</span>, Mose Sakashita, Jingjin Li, Upol Ehsan, and Andrea Stevenson Won</h4>             
                    <h3 class="record"><i class="fa fa-book"></i>Published in ACM CHI 2020 (full paper)</h3>
                    <h3>We describe ReliveInVR, a new time-machine-like VR experience sharing method. ReliveInVR allows multiple users to immerse themselves in the relived experience together and independently view the experience from any perspective. We conducted a 1x3 within-subject study with 26 dyads to compare ReliveInVR with (1) co-watching 360-degree videos on desktop, and (2) co-watching 360-degree videos in VR. Our results suggest that participants reported higher levels of immersion and social presence in ReliveInVR. Participants in ReliveInVR also understood the shared experience better, discovered unnoticed things together and found the sharing experience more fulfilling.</h3>
                    <ul class="list-btn">
                        <li><a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376642" target="_blank" class="btn-paper"><i class="fa fa-caret-square-o-right"></i>Paper</a></li>
                        <li><a href="https://www.youtube.com/watch?v=d2WQXduv6oU" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>Video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- RelivingVRV2 project -->
            <!-- ReliveReality project-->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style small-padding">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/ReliveReality_Idea.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/ReliveReality_final.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/ReliveReality_P1.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/ReliveReality_P2.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/ReliveReality_P3.png"></div>                     
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">IEEE VR 2020</header>ReliveReality: Enabling Socially Reliving Experiences in Virtual Reality via a Single RGB camera
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang</span>, Shengguang Bai, Ian Switzer, and Andrea Stevenson Won</h4>             
                    <h3 class="record"><i class="fa fa-book"></i>Published in IEEE VR 2020 (poster)</h3>
                    <h3>We present a new experience sharing method, ReliveReality, which  transforms traditional photo/video memories into 3D reconstructed memories and allows users to relive past experiences through VR socially. ReliveReality utilizes deep learning-based computer vision techniques to reconstruct people in clothing, estimate multi-person 3D pose and reconstruct 3D environments with only a single RGB camera. Integrating with a networked multi-user VR environment, ReliveReality enables people to ‘enter' into a past experience, move around and relive a memory from different perspectives in VR together. We discuss the technical implementation and implications of such techniques for privacy}</h3>
                    <ul class="list-btn">
                        <li><a href="https://ieeexplore.ieee.org/document/9090534" target="_blank" class="btn-paper"><i class="fa fa-caret-square-o-right"></i>Paper</a></li>
                        <li><a href="https://youtu.be/nojZW9Ovb9A" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>Video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- ReliveReality project -->
            <!-- RelivingVR project-->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style small-padding">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/VR-Relive.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/VR_interactions_new.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/study_conditions.png"></div>                    
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">IEEE VR 2019</header>
                        RelivelnVR: Capturing and Reliving Virtual Reality Experiences Together
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang</span>, Mose Sakashita, Upol Ehsan, Jingjin Li, and Andrea Stevenson Won</h4>             
                    <h3 class="record"><i class="fa fa-book"></i>Published in IEEE VR 2019 (poster)</h3>
                    <h3>We present a new type of sharing VR experience over distance which allows people to relive their recorded experience in VR together. We describe a pilot study examining the user experience when people share their VR experience together remotely. Finally, we discuss the implications for sharing VR experiences over time and space.</h3>
                    <ul class="list-btn">
                        <!--<li><a href="#workModal3" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>-->
                        <li><a href="https://ieeexplore.ieee.org/document/8798363" target="_blank" class="btn-paper"><i class="fa fa-caret-square-o-right"></i>Paper</a></li>
                        <li><a href="https://drive.google.com/open?id=18EynpMwK95Pgz2mEIDSRDIZT276INYTx" target="_blank" class="btn-paper"><i class="fa fa-caret-square-o-right"></i>Poster</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- RelivingVR project -->
            <!-- VR-Replay project-->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style small-padding">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/VRReplay_Prototype_System.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/VRReplay_User_Study.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/VR_Interaction2.png"></div>                    
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">IEEE VR 2019</header>
                        VR-Replay: Capturing and Replaying Avatars in VR for Asynchronous 3D Collaborative Design
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang</span>, Logan Drumm, Christopher Troup,  Yingjie Ding, and  Andrea Stevenson Won</h4>             
                    <h3 class="record"><i class="fa fa-book"></i>Published in IEEE VR 2019 (poster)</h3>
                    <h3>Distributed teams rely on asynchronous CMC tools to complete collaborative tasks due to the difficulties and costs surrounding scheduling synchronous communications. In this paper, we present VR-Replay, a new communication tool that records and replays avatars with both nonverbal behavior and verbal communication in VR asynchronous collaboration.</h3>
                    <ul class="list-btn">
                        <!--<li><a href="#workModal3" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>-->
                        <li><a href="https://ieeexplore.ieee.org/document/8797789" target="_blank" class="btn-paper"><i class="fa fa-caret-square-o-right"></i>Paper</a></li>
                        <li><a href="https://www.youtube.com/watch?v=-XGJGYP4AHY" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>Video</a></li>
                        <li><a href="https://drive.google.com/open?id=1Y9sUHleRLqJD5zL7GDu0nC2JwsI7KAQ0" target="_blank" class="btn-paper"><i class="fa fa-caret-square-o-right"></i>Poster</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- VR-Replay project -->
            <!-- droneIO project -->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style">                    
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/droneio_1.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/droneio_2.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/droneio_3.png"></div>
                    </div>                                       
                </div>
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">IEEE HRI 2019</header>
                        Drone.io: A Gestural and Visual Interface for Human-Drone Interaction
                    </h2>
                    <h4><i class="fa fa-users"></i>Jessica R. Cauchard, Alex Tamkin, <span class="bold">Cheng-Yao Wang,</span>, Luke Vink, Michelle Park, Tommy Fang, and James A. Landay</h4>
                    
                    <h3 class="record"><i class="fa fa-book"></i>Published in HRI 2019 (full paper)</h3>
                    <h3> We introduce drone.io, a projected body-centric graphical user interface for human-drone interaction. Using two simple gestures, users can interact with a drone in a natural manner. drone.io is the first human-drone graphical user interface embedded on a drone to provide both input and output capabilities. This paper describes the design process of drone.io. We report drone.io's evaluation in three user studies (N=27) and show that people were able to use the interface with little prior training.</h3>
                    
                    <ul class="list-btn">
                        <li><a href="https://ieeexplore.ieee.org/document/8673011" class="btn-paper" data-toggle="lightbox"><i class="fa fa-file-text"></i>Paper</a></li>                          
                    </ul>                   
                </div>
            </div>
            <!-- droneIO project -->
            <!-- RoMA project-->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style small-padding">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/RoMA_img1.jpg"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/RoMA_img5.jpg"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/RoMA_img4.jpg"></div>
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">CHI 2018</header>
                        RoMA: Interactive Fabrication with Augmented Reality and a Robotic 3D Printer
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i>Huaishu Peng, <span class="bold">Cheng-Yao Wang</span>*, Jimmy Briggs*, Kevin Guo, Joseph Kider, Stefanie Mueller, Patrick Baudisch, François Guimbretière. (*equal contribution)</h4>
                    <h3 class="record"><i class="fa fa-book"></i>Published in CHI 2018 (full paper)</h3>             
                    <h3>We present the Robotic Modeling Assistant (RoMA), an interactive fabrication system providing a fast, precise, hands-on and in-situ modeling experience with an augmented reality CAD editor and a robotic arm 3D printer. With RoMA, users can integrate real-world constraints into a design rapidly, allowing them to create well-proportioned tangible artifacts. Users can even directly design on and around an existing object, and extending the artifact by in-situ fabrication.</h3>
                    <ul class="list-btn">
                        <!--<li><a href="#workModal3" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>-->
                        <li><a href="http://www.huaishu.me/projects/roma.pdf" target="_blank" class="btn-paper"><i class="fa fa-file-text"></i>paper</a></li>                       
                        <li><a href="https://www.youtube.com/watch?v=K_wWuYD1Fkg" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- RoMA project -->
            <!-- Reduct project-->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style small-padding">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/reduct_img1.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/reduct_img2.png"></div>
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">CHI 2017</header>
                        Teaching Programming with Gamified Semantics
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i>Ian Arawjo, <span class="bold">Cheng-Yao Wang,</span> Andrew C. Myers, Erik Andersen, and François Guimbretière</h4>
                    <h3 class="record"><i class="fa fa-book"></i>Published in CHI 2017 (full paper)</h3>             
                    <h3>We present Reduct, an educational game embodying a new, comprehension-first approach to teaching novices core programming concepts which include functions, Booleans, equality, conditionals, and mapping functions over sets. In this novel teaching strategy, the player executes code using reduction-based operational semantics. During gameplay, code representations fade from concrete, block-based graphics to the actual syntax of JavaScript ES2015. Our study result shows that novices demonstrated promising learning of core concepts expressed in actual JavaScript code in a short timeframe with our Reduct game.</h3>
                    <ul class="list-btn">
                        <!--<li><a href="#workModal3" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>-->
                        <li><a href="https://dl.acm.org/citation.cfm?id=3025711" target="_blank" class="btn-paper"><i class="fa fa-file-text"></i>paper</a></li>                       
                        <li><a href="https://www.youtube.com/watch?v=vSXBA95uQ04" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- Reduct project -->
            <!-- PalmType project-->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style small-padding">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/p-t-palm.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/p-t-img1.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/p-t-img3.jpg"></div>
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">MobileHCI 2015</header>
                        PalmType: Using Palms as Keyboards for Smart Glasses
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Wei-Chen Chu, Po-Tsung Chiu, Min-Chieh Hsiu, Yih-Harn Chiang, Mike Y. Chen</h4>
                    <h3 class="record"><i class="fa fa-book"></i>Published in MobileHCI 2015 (full paper)</h3>             
                    <h3>We present PalmType, which uses palms as interactive keyboards for smart wearable displays like Google Glass. PalmType leverages user's innate ability to pinpoint a specific area of palm and fingers without visual attention (i.e. proprioception) and provides visual feedback via wearable displays. With wrist-worn sensors and wearable displays, PalmType enables typing without requiring users to hold any devices and visual attention to their hands. We conducted design sessions with 6 participants to see how users map QWERTY layout to their hands based on their proprioception.</h3>
                    <ul class="list-btn">
                        <li><a href="#workModal3" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>
                        <li><a href="http://dl.acm.org/citation.cfm?id=2785886" target="_blank" class="btn-paper"><i class="fa fa-file-text"></i>paper</a></li>                       
                        <li><a href="https://www.youtube.com/watch?v=mwjADJ-v8nU" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- PalmType project -->
            <!-- PalmGesture project-->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/p-g-app1.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/p-g-img1.jpg"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/p-g-app3.png"></div>
                    </div>                    
                </div>                        
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con3">MobileHCI 2015</header>
                        PalmGesture: Using Palms as Gesture Interfaces for Eyes-free Input
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Min-Chieh Hsiu, Po-Tsung Chiu, Chiao-Hui Chang, Liwei Chan, Bing-Yu Chen, Mike Y. Chen</h4>                     
                    <h3 class="record"><i class="fa fa-book"></i>Published in MobileHCI 2015 (full paper)</h3>
                    <h3>With abundant tactile cues and proprioception on palms, the palm can be leveraged as an interface for eyes-free input which decreases visual attention to interfaces and minimizes cognitive/physical effort. We explored eyes-free gesture interactions on palms that enables users to interact with devices by drawing stroke gestures on palms without looking at palms. To understand user behavior when users draw gestures on palms and how gestures on palms are affected by palm characteristics, we conducted two 24-participant user studies. Also, we implemented EyeWrist that turns the palm into a gesture interface by embedding a micro-camera and an IR laser line generator on the wristband, and three interaction techniques that takes advantages of palm characteristics are proposed. </h3>
                    <ul class="list-btn">
                        <li><a href="#workModal5" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>
                        <li><a href="http://dl.acm.org/citation.cfm?id=2785885" target="_blank" class="btn-paper"><i class="fa fa-file-text"></i>paper</a></li>                        
                        <li><a href="https://www.youtube.com/watch?v=Ia7kr6-lRYw" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- PalmGesture project -->
            <!-- EverTutor project -->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-t-gestures.jpg"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-t-browse.jpg"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-t-create.jpg"></div>
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con1">CHI 2014</header>
                        EverTutor: Automatically Creating Interactive Guided Tutorials on Smartphones by User Demonstration
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Wei-Chen Chu, Hou-Ren Chen, Chun-Yen Hsu, Mike Y. Chen</h4>
                    <h3 class="record"><i class="fa fa-book"></i>Published in CHI 2014 (full paper)</h3>
                    <h3>We present EverTutor that automatically generates interactive tutorials on smartphone from user demonstration. It simplifies the tutorial creation, provides tutorial users with contextual step-by-step guidance and avoids the frequent context switching between tutorials and users' primary tasks. In order to generate tutorials automatically, EverTutor records low-level touch events to detect gestures and identify on-screen targets. When a tutorial is browsed, the system uses vision-based techniques to locate the target regions and overlays the corresponding input prompt contextually. It also identifies the correctness of users' interaction to guide the users step by step.</h3>
                    <ul class="list-btn">
                        <li><a href="#workModal2" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>
                        <li><a href="https://dl.acm.org/citation.cfm?id=2557407" target="_blank" class="btn-paper"><i class="fa fa-file-text"></i>paper</a></li>
                        <li><a href="https://www.youtube.com/watch?v=IubKeveE0Xg" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- EverTutor project -->
            <!-- RealSense project -->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/r-s-img1.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/r-s-logo.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/r-s-img2.png"></div>
                    </div>                    
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>
                        <header class="con2">MM 2013</header>
                        RealSense: Directional Interaction for Proximate Mobile Sharing
                    </h2>
                    <h4 class="m-top"><i class="fa fa-users"></i>Chien-Pang Lin, <span class="bold">Cheng-Yao Wang,</span>Hou-Ren Chen, Wei-Chen Chu, Mike Y. Chen</h4>
                    <h3 class="record"><i class="fa fa-book"></i>Published in MM 2013 (short paper)</h3>
                    <h3>We present RealSense, a technology that enables users to easily share media files with proximate users by detecting the relative direction of each other only with built-in orientation sensors on smartphones. With premise that users are arranged as a circle and every user is facing the center of that circle, RealSense continuously collects the directional heading of each phone to calculate the virtual position of each user in real time during the sharing.</h3>
                    <ul class="list-btn">
                        <li><a href="#workModal1" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>
                        <li><a href="https://dl.acm.org/citation.cfm?id=2502202" target="_blank" class="btn-paper"><i class="fa fa-file-text"></i>paper</a></li>
                        <li><a href="https://www.youtube.com/watch?v=p5gehFvSIE8" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- RealSense project --> 
            <!-- EyeWrist project -->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-w-prototype2.jpg"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-w-scenario1.jpg"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-w-prototype.jpg"></div>
                    </div>                                       
                </div>                            
                <div class="col-md-7 text-left text-style">
                    <h2>EyeWrist: Enabling Gesture-Based Interaction on Palm with a Wrist-Worn Sensor</h2>
                    <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Po-Tsung Chiu, Min-Chieh Hsiu, Chiao-Hui Chang, Mike Y. Chen</h4>
                    <h3><span class="span-focus"><i class="fa fa-trophy"></i>Excellent Work,</span>The 8th Acer Long-Term Smile Innovation Contest, 2014</h3>
                    <h3>We present EyeWrist, which uses palms as the gesture interface for smart wearable displays such as Google Glass. With abundant tactile cues and proprioception on palms, EyeWrist can also be leveraged for device-less and eyes-free remote for smart TVs. EyeWrist embeds a micro-camera and an IR laser line generator on the wristband and use computer vision algorithms to calculate the finger’s position on the palm. Without requiring the line of sight of users’ fingertips on palms, the camera height could be lower, making the whole device more portable. We also implemented a gesture recognizer to distinguish different symbols, letters or touchscreen gestures(e.g. swipe, pinch) on palms. The recognition result would be sent to smart devices via Wi-Fi for gesture-based interaction.</h3>
                    <ul class="list-btn">
                        <li><a href="#workModal4" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>
                        <li><a href="/" target="_blank" class="btn-paper"><i class="fa fa-file-text"></i>poster</a></li>
                        <li><a href="https://www.youtube.com/watch?v=WV7VwQw9OPM" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- EyeWrist project -->
            <!-- EyeWatch project -->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style">
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-wa-logo.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-wa-img1.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/e-wa-img2.png"></div>
                    </div>                    
                </div>
                <div class="col-md-7 text-left text-style">
                    <h2>EyeWatch: Touch Interaction on Back of the Hand for Smart Watches</h2>
                    <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Po-Tsung Chiu, Min-Chieh Hsiu</h4>
                    <h3><span class="span-focus"><i class="fa fa-trophy"></i>2nd Prize,</span>MobileHero – user experience design competition, 2014</h3>                      
                    <h3>We present EyeWatch, which uses back of the hand as gesture interface for smart watches. EyeWatch not only overcomes the Big smartwatch problem: occlusion and fat finger problem, but also enables more powerful and natural interaction such as drawing a symbol quickly to open an application, or intuitively handwriting on back of hand to input message. Our proof-of-concept implementation consists of a micro-camera and an IR laser line generator on the smart watch, and computer vision algorithms are used to calculate the finger’s position on the back of hand.</h3>
                    <ul class="list-btn">
                        <li><a href="#workModal6" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>                        
                        <li><a href="https://www.youtube.com/watch?v=j-BhWbpgHZk" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- EyeWatch project -->
            <!-- Shrinking project -->
            <div class="row v-center">
                <div class="col-md-5 text-center img-style">                    
                    <div class="my-slide">
                        <!-- Wrapper for slides -->
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/s-a-screen.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/s-a-sketch.png"></div>
                        <div><img class="img-responsive img-centered" data-lazy="img/projects/s-a-car2.jpg"></div>
                    </div>                                       
                </div>
                <div class="col-md-7 text-left text-style">
                    <h2>The Incredible Shrinking Adventure</h2>
                    <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Min-Chieh Hsiu, Chin-Yu Chien, Shuo Yang</h4>
                    <h3><span class="span-focus"><i class="fa fa-trophy"></i>Final Shortlist,</span>ACM UIST 2014 Student Innovation Contest</h3>
                    <h3>Imagining you were shrunk, you can explore your big house, play with big pets and family. To fulfill the imagination and provide users with incredible shrinking adventures, we use a robotic car and a google cardboard which turns a smartphone into a VR headset. We build a robotic car and attach a smartphone on the pan/tilt servo bracket. stereo images are generated from smartphone’s camera and are streamed to the other smartphone inside of the google cardboard. When users see the world through the smartphone on robotic car, they feel they were shrunk.</h3>
                    <ul class="list-btn">
                        <li><a href="#workModal7" class="btn-more" data-toggle="modal"><i class="fa fa-plus-square"></i>more info</a></li>
                        <li><a href="img/projects/poster-lilliput_small.png" class="btn-paper" data-toggle="lightbox"><i class="fa fa-file-text"></i>poster</a></li>
                        <li><a href="https://www.youtube.com/watch?v=UPvq2fLKTsQ" target="_blank" class="btn-video"><i class="fa fa-caret-square-o-right"></i>video</a></li>                          
                    </ul>                    
                </div>
            </div>
            <!-- Shrinking project -->                                   
        </div>
    </section>

    <!-- skill section -->
    <section id="skills">
        <div class="container">
            <h1>my skills</h1>
            <div class="row">
                <div class="col-md-4 skill-text">
                    <h2>Computer Science</h2>
                    <ul>
                        <li>C#, Python, C/C++, JAVA, Javascript</li>
                        <li>Deep learning techniques in Computer Vision</li>
                        <li>Human Digitalization, 3D human pose estimation, 3D reconstruction</li>
                    </ul>
                </div>
                <div class="col-md-4 skill-text">
                    <h2>Virtual Reality</h2>
                    <ul>
                        <li>Expertise in Unity3D and VR projects development</li>
                        <li>Solid experiences in VR projects development</li>
                        <li>3D Avatar creation, rigging, animation</li>
                    </ul>
                </div>
                <div class="col-md-4 skill-text">
                    <h2>Research Design</h2>
                    <ul>
                        <li>Mixed-method research design</li>
                        <li>Quantitative analysis using R</li>
                        <li>Questionnaire design and qualitative analysis</li>
                    </ul>
                </div>
            </div>
        </div>
    </section>

    <!-- Publication Section -->
    <section id="publication">
        <div class="container">
            <div class="row">
                <div class="col-lg-5 text-center col-left-style">
                    <h1>publications</h1>
                    <div class="list-wrap text-left">
                        <h2>Again, Together: Socially Reliving Virtual Reality Experiences When Separated</h2>
                        <h3>CHI 2020, Full Paper</h3>
                         <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang</span>, Mose Sakashita, Jingjin Li, Upol Ehsan, and Andrea Stevenson Won</h4>              
                    </div>
                    <div class="list-wrap text-left">
                        <h2>ReliveReality: Enabling Socially Reliving Experiences in Virtual Reality via a Single RGB camera</h2>
                        <h3>IEEE VR 2020, Poster</h3>
                        <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang</span>, Shengguang Bai, and Andrea Stevenson Won</h4>             
                    </div>
                    <div class="list-wrap text-left">
                        <h2>Privacy-Preserving Relived Experiences in Virtual Reality</h2>
                        <h3>IEEE VR 2020, Doctoral Consortium</h3>
                         <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang</span></h4>              
                    </div>
                    <div class="list-wrap text-left">
                        <h2><a href="https://ieeexplore.ieee.org/document/8797789" target="_blank">RelivelnVR: Capturing and Reliving VirtualReality Experiences Together</a></h2>
                        <h3>IEEE VR 2019, Poster</h3>
                         <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang</span>, Mose Sakashita, Upol Ehsan, Jingjin Li, and Andrea Stevenson Won</h4>              
                    </div>
                    <div class="list-wrap text-left">
                        <h2><a href="https://ieeexplore.ieee.org/document/8797789" target="_blank">VR-Replay: Capturing andReplaying Avatars in VR for Asynchronous 3D Collaborative Design</a></h2>
                        <h3>IEEE VR 2019, Poster</h3>
                         <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang</span>, Logan Drumm, Christopher Troup, Yingjie Ding, and Andrea Stevenson Won</h4>              
                    </div>
                    <div class="list-wrap text-left">
                        <h2><a href="https://ieeexplore.ieee.org/document/8673011" target="_blank">Drone.io: A Gestural and Visual Interface for Human-DroneInteraction</a></h2>
                        <h3>IEEE HRI 2019, Full Paper</h3>
                         <h4><i class="fa fa-users"></i>Jessica R. Cauchard, Alex Tamkin, <span class="bold">Cheng-Yao Wang</span>, Luke Vink, Michelle Park, Tommy Fang, and James A. Landay</h4>              
                    </div>
                    <div class="list-wrap text-left">
                        <h2><a href="http://www.huaishu.me/projects/roma.pdf" target="_blank">RoMA: Interactive Fabrication with Augmented Reality and a Robotic 3D Printer</a></h2>
                        <h3>ACM CHI 2018, Full Paper</h3>
                         <h4><i class="fa fa-users"></i>Huaishu Peng, <span class="bold">Cheng-Yao Wang</span>*, Jimmy Briggs*, Kevin Guo, Joseph Kider, Stefanie Mueller, Patrick Baudisch, François Guimbretière. (*equal contribution)</h4>              
                    </div>
                    <div class="list-wrap text-left">
                        <h2><a href="https://dl.acm.org/citation.cfm?id=3025711" target="_blank">Teaching Programming with Gamified Semantics</a></h2>
                        <h3>ACM CHI 2017, Full Paper</h3>
                         <h4><i class="fa fa-users"></i>Ian Arawjo, <span class="bold">Cheng-Yao Wang</span>, Andrew C. Myers, Erik Andersen, and François Guimbretière</h4>              
                    </div>
                    <div class="list-wrap text-left">
                        <h2><a href="http://dl.acm.org/citation.cfm?id=2785886" target="_blank">PalmType: Using Palms as Keyboards for Smart Glasses</a></h2>
                        <h3>ACM MobileHCI 2015, Full Paper</h3>
                         <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Wei-Chen Chu, Po-Tsung Chiu, Min-Chieh Hsiu, Yih-Harn Chiang, Mike Y. Chen</h4>              
                    </div>
                    <div class="list-wrap text-left">
                        <h2><a href="http://dl.acm.org/citation.cfm?id=2785885" target="_blank">PalmGesture: Using Palms as Gesture Interfaces for Eyes-free Input</a></h2>
                        <h3>ACM MobileHCI 2015, Full Paper</h3>
                        <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Min-Chieh Hsiu, Po-Tsung Chiu, Chiao-Hui Chang, Liwei Chan, Bing-Yu Chen, Mike Y. Chen</h4>              
                    </div> 
                    <div class="list-wrap text-left">
                        <h2><a href="https://dl.acm.org/citation.cfm?id=2557407" target="_blank">EverTutor: Automatically Creating Interactive Guided Tutorials on Smartphones by User Demonstration</a></h2>
                        <h3>ACM CHI 2014, Full Paper</h3>
                        <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Wei-Chen Chu, Hou-Ren Chen, Chun-Yen Hsu, Mike Y. Chen</h4>
                    </div>
                    <div class="list-wrap text-left">
                        <h2><a href="https://dl.acm.org/citation.cfm?id=2502202" target="_blank">RealSense: Directional Interaction for Proximate Mobile Sharing Using Built-in Orientation Sensors</a></h2>
                        <h3>MM 2013, short paper</h3>
                        <h4><i class="fa fa-users"></i>Chien-Pang Lin, <span class="bold">Cheng-Yao Wang,</span>Hou-Ren Chen, Wei-Chen Chu, Mike Y. Chen</h4>
                    </div>                  
                </div>
                <div class="col-lg-7 text-center col-right-style">
                    <h1>awards</h1>
                    <div class="list-wrap text-left">
                        <h2>RelivelnVR: Capturing and Reliving VirtualReality Experiences Together</h2>
                        <h3><span class="span-focus"><i class="fa fa-trophy"></i>Best Poster Honorable Mention,</span>In 2019 IEEE Conference onVirtual Reality and 3D User Interfaces (VR), 2019</h3>
                        <h3><span class="span-focus"><i class="fa fa-trophy"></i>Selected poster,</span>The Cornell CIS 20th Anniversary Reception, 2019</h3>  
                    </div>
                    <div class="list-wrap text-left">
                        <h2>EverTutor: Automatically Creating Interactive Guided Tutorials on Smartphones by User Demonstration</h2>
                        <h3><span class="span-focus"><i class="fa fa-trophy"></i>1st Prize,</span>The 11th Deep Innovations with Impact, National Taiwan University, 2013</h3>
                        <h3><span class="span-focus"><i class="fa fa-trophy"></i>1st Prize,</span>The 11th Y.S. National Innovation Software Application Contest, 2014</h3>  
                    </div>
                    <div class="list-wrap text-left">
                        <h2>MagicWrist - Connect the world</h2>
                        <h3><span class="span-focus"><i class="fa fa-trophy"></i>Excellent Work,</span>The 8th Acer Long-Term Smile Innovation Contest, 2014</h3>            
                    </div>
                    <div class="list-wrap text-left">
                        <h2>The Incredible Shrinking Adventure</h2>
                        <h3><span class="span-focus"><i class="fa fa-trophy"></i>Final Shortlist,</span>ACM UIST 2014 Student Innovation Contest</h3>                      
                    </div>
                    <div class="list-wrap text-left">
                        <h2>EyeWatch: Touch Interaction on Back of the Hand for Smart Watches</h2>
                        <h3><span class="span-focus"><i class="fa fa-trophy"></i>2nd Prize,</span>MobileHero – user experience design competition, 2014</h3>               
                    </div>
                    <div class="list-wrap text-left">
                        <h2>EyeWrist - Enabling Gesture-Based Interaction on Palm with a Wrist-Worn Sensor</h2>
                        <h3><span class="span-focus"><i class="fa fa-trophy"></i>Final Shortlist,</span>MediaTek Wearable device into IoT world competition, 2014</h3>               
                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer>
        <div class="container">
            <div class="row">
                <div class="col-md-4">
                    <h3>Copyright &copy; 2014 Eric Wang</h3>
                </div>
                <div class="col-md-4">
                    <ul class="icon-list">
                        <li><a href="mailto:ericwang0701@gmail.com"><i class="fa fa-envelope-square fa-3x"></i></a></li>
                        <li><a href="https://www.facebook.com/profile.php?id=100000233739748" target="_blank"><i class="fa fa-facebook-square fa-3x"></i></a></li>
                        <li><a href="https://www.youtube.com/channel/UCbBZl_n5DbM4i8xhXJrdRIg" target="_blank"><i class="fa fa-youtube-square fa-3x"></i></a></li>
                        <li><a href="https://www.linkedin.com/in/ericwanghci" target="_blank"><i class="fa fa-linkedin-square fa-3x"></i></a></li>
                    </ul>
                </div>
                <div class="col-md-4">
                    <h3>Last Update: Nov. 2019</h3>
                </div>
            </div>
        </div>
    </footer>

    <!-- Project Modals -->    

    <!-- Project Modal 1 -->
    
    <div class="work-modal modal fade" id="workModal1" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="modal-body">                    
                    <div class="row">
                        <div class="col-sm-8 col-sm-offset-2 text-left">
                            <div class="modal-title text-center">
                                <h2>RealSense:<br>Directional Interaction for Proximate Mobile Sharing</h2>
                            </div>                                                                                   
                            <div class="item active embed-responsive embed-responsive-16by9 video-wraper">                                
                                <iframe src="https://www.youtube.com/embed/p5gehFvSIE8" frameborder="0" allowfullscreen></iframe>                                
                            </div>                                                                                                  
                            <h4><i class="fa fa-users"></i>Chien-Pang Lin, <span class="bold">Cheng-Yao Wang,</span>Hou-Ren Chen, Wei-Chen Chu, Mike Y. Chen</h4>
                            <h3 class="text-center">Published in <span class="span-focus">MM 2013</span>(short paper)</h3>
                            <h3>We present RealSense, a technology that enables users to easily share media files with proximate users by detecting the relative direction of each other only with built-in orientation sensors on smartphones. With premise that users are arranged as a circle and every user is facing the center of that circle, RealSense continuously collects the directional heading of each phone to calculate the virtual position of each user in real time during the sharing.</h3>
                            <h3>RealSense users can simply share photos to others by swiping and throwing the photo to another user’s direction without remembering or searching the name and even the device id of a specific receiver. We evaluated the orientation sensor error and the minimal arc degree for selection, and compared RealSense with linear menu, pie menu and NFC. Our results show that the limitation that requires participants to face toward the circle center is rather acceptable and participants preferred RealSense than other sharing interactions especially they were unacquainted with each other.</h3>                                                                                                             
                        </div>
                    </div>
                    <div class="img-wraper text-center">
                        <img class="img-responsive img-centered" src="img/projects/r-s-img3.jpg">
                        <img class="img-responsive img-centered" src="img/projects/r-s-img4.jpg">
                    </div>
                    <button type="button" class="btn btn-primary" data-dismiss="modal"><i class="fa fa-times"></i> Close Project</button>
                </div>
            </div>
        </div>
    </div>

    <!-- Project Modal 2 -->

    <div class="work-modal modal fade" id="workModal2" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="modal-body">                    
                    <div class="row">
                        <div class="col-sm-8 col-sm-offset-2 text-left">
                            <div class="modal-title text-center">
                                <h2>EverTutor:<br>Automatically Creating Interactive Guided Tutorials on Smartphones by User Demonstration</h2>
                            </div>
                            <div class="item active embed-responsive embed-responsive-16by9 video-wraper">
                                <iframe src="https://www.youtube.com/embed/IubKeveE0Xg" frameborder="0" allowfullscreen></iframe>                                
                            </div>                                                                                                  
                            <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Wei-Chen Chu, Hou-Ren Chen, Chun-Yen Hsu, Mike Y. Chen</h4>
                            <h3>Published in <span class="span-focus">CHI 2014</span>(full paper)</h3>
                            <h3>We present EverTutor, a system that automatically generates interactive tutorials on smartphone from user demonstration. For tutorial authors, it simplifies the tutorial creation. For tutorial users, it provides contextual step-by-step guidance and avoids the frequent context switching between tutorials and users' primary tasks. In order to generate the tutorials automatically, EverTutor records low-level touch events to detect gestures and identify on-screen targets. When a tutorial is browsed, the system uses vision-based techniques to locate the target regions and overlays the corresponding input prompt contextually. It also identifies the correctness of users' interaction to guide the users step by step.</h3>
                            <h3>We conducted a 6-person user study for creating tutorials and a 12-person user study for browsing tutorials, and we compared EverTutor's interactive tutorials to static and video ones. Study results show that creating tutorials by EverTutor is simpler and faster than producing static and video tutorials. Also, when using the tutorials, the task completion time for interactive tutorials were 3-6 times faster than static and video tutorials regardless of age group. In terms of user preference, 83% of the users chose interactive type as the preferred tutorial type and rated it easiest to follow and easiest to understand.</h3>                                                       
                        </div>
                    </div>
                    <button type="button" class="btn btn-primary" data-dismiss="modal"><i class="fa fa-times"></i> Close Project</button>
                </div>
            </div>
        </div>
    </div>

    <!-- Project Modal 3 -->

    <div class="work-modal modal fade" id="workModal3" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="modal-body">                      
                    <div class="row">
                        <div class="col-sm-8 col-sm-offset-2 text-left">
                            <div class="modal-title text-center">
                                <h2>PalmType: <br>Using Palms as Keyboards for Smart Glasses</h2>
                            </div>
                            <div class="item active embed-responsive embed-responsive-16by9 video-wraper">
                                <iframe src="https://www.youtube.com/embed/mwjADJ-v8nU" frameborder="0" allowfullscreen></iframe>                                
                            </div>                                                                                                  
                            <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Wei-Chen Chu, Po-Tsung Chiu, Yih Harn Chiang, Mike Y. Chen</h4>  
                            <h3>We present PalmType, which uses palms as interactive key- boards for smart wearable displays, such as Google Glass. PalmType leverages users’ innate ability to pinpoint specific areas of their palms and fingers without visual attention (i.e. proprioception), and provides visual feedback via the wear- able displays. With wrist-worn sensors and wearable dis- plays, PalmType enables typing without requiring users to hold any devices and does not require visual attention to their hands. We conducted design sessions with 6 participants to see how users map QWERTY layout to their hands based on their proprioception.</h3>
                            <h3>To evaluate typing performance and preference, we conducted a 12-person user study using Google Glass and Vicon motion tracking system, which showed that PalmType with optimized QWERTY layout is 39% faster than current touchpad-based keyboards. In addition, PalmType is preferred by 92% of the participants. We demonstrate the feasibility of wearable PalmType by building a prototype that uses a wrist-worn array of 15 infrared sensors to detect users’ finger position and taps, and provides visual feedback via Google Glass.</h3>                                                       
                        </div>
                    </div>
                    <button type="button" class="btn btn-primary" data-dismiss="modal"><i class="fa fa-times"></i> Close Project</button>
                </div>
            </div>
        </div>
    </div>

    <!-- Project Modal 4 -->

    <div class="work-modal modal fade" id="workModal4" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="modal-body">                    
                    <div class="row">
                        <div class="col-sm-8 col-sm-offset-2 text-left">
                            <div class="modal-title text-center">
                                <h2>EyeWrist: <br>Enabling Gesture-Based Interaction on Palm with a Wrist-Worn Sensor</h2>
                            </div>
                            <div class="item active embed-responsive embed-responsive-16by9 video-wraper">
                                <iframe src="https://www.youtube.com/embed/WV7VwQw9OPM" frameborder="0" allowfullscreen></iframe>                                
                            </div>                                                                                                  
                            <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Po-Tsung Chiu, Min-Chieh Hsiu, Chiao-Hui Chang, Mike Y. Chen</h4>
                            <h3><span class="span-focus"><i class="fa fa-trophy"></i>Excellent Work,</span>The 8th Acer Long-Term Smile Innovation Contest, 2014</h3>   
                            <h3>We present EyeWrist, which uses palms as the gesture interface for smart wearable displays such as Google Glass. With abundant tactile cues and proprioception on palms, EyeWrist can also be leveraged for device-less and eyes-free remote for smart TVs. EyeWrist embeds a micro-camera and an IR laser line generator on the wristband and use computer vision algorithms to calculate the finger’s position on the palm. Without requiring the line of sight of users’ fingertips on palms, the camera height could be lower, making the whole device more portable. We also implemented a gesture recognizer to distinguish different symbols, letters or touchscreen gestures(e.g. swipe, pinch) on palms. The recognition result would be sent to smart devices via Wi-Fi for gesture-based interaction.</h3>                                                       
                        </div>
                    </div>
                    <button type="button" class="btn btn-primary" data-dismiss="modal"><i class="fa fa-times"></i> Close Project</button>
                </div>
            </div>
        </div>
    </div>

    <!-- Project Modal 5 -->

    <div class="work-modal modal fade" id="workModal5" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="modal-body">                                         
                    <div class="row">
                        <div class="col-sm-8 col-sm-offset-2 text-left">
                            <div class="modal-title text-center">
                                <h2>PalmGesture: <br>Using Palms as Gesture Interfaces for Eyes-free Input</h2>
                            </div>
                            <div class="item active embed-responsive embed-responsive-16by9 video-wraper">
                                <iframe src="https://www.youtube.com/embed/Ia7kr6-lRYw" frameborder="0" allowfullscreen></iframe>                                
                            </div>                                                                                                  
                            <h3>Submitted to ACM MobileHCI 2015, Full Paper</h3> 
                            <h3>With abundant tactile cues and proprioception on palms, the palm can be leveraged as an interface for eyes-free input which decreases visual attention to interfaces and minimizes cognitive/physical effort. In this paper, we explored eyes-free gesture interactions on palms, which enables users to interact with devices by drawing stroke gestures on palms without looking at palms. In two 24-person user studies, we observed that (1) users preferred using the whole palm region as the gesture interface with 3 categories of hand orientations. (2) users tended to draw different gestures from the same start region due to proprioception on palms. (3) the accuracy of eyes-free Graffiti input and multi-stroke gesture recognizer on palm interface reached as high as 98% and 95% respectively. (4) the tactile feedback of palms can significantly help participants build up spatial memory of gestures. Also, we implemented EyeWrist that turns the palm into a gesture interface by em- bedding a micro-camera and an IR laser line generator on the wristband, and three interaction techniques that takes advantages of palm characteristics are proposed. The preliminary evaluation revealed that EyeWrist enabled users to draw graffiti letter and multi-stroke gestures with 90% above accuracy, and that the concept of eyes-free palm-based gesture interaction was appealing to users.</h3>                                                       
                        </div>
                    </div>
                    <button type="button" class="btn btn-primary" data-dismiss="modal"><i class="fa fa-times"></i> Close Project</button>
                </div>
            </div>
        </div>
    </div>

    <!-- Project Modal 6 -->

    <div class="work-modal modal fade" id="workModal6" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="modal-body">                                        
                    <div class="row">
                        <div class="col-sm-8 col-sm-offset-2 text-left">
                            <div class="modal-title text-center">
                                <h2>EyeWatch: <br>Touch Interaction on Back of the Hand for Smart Watches</h2>
                            </div>
                            <div class="item active embed-responsive embed-responsive-16by9 video-wraper">
                                <iframe src="https://www.youtube.com/embed/j-BhWbpgHZk" frameborder="0" allowfullscreen></iframe>                                
                            </div>                                                                                                  
                            <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Po-Tsung Chiu, Min-Chieh Hsiu</h4>
                            <h3><span class="span-focus">12/03 Final,</span>MobileHero – user experience design competition, 2014</h3> 
                            <h3>We present EyeWatch, which uses back of the hand as gesture interface for smart watches. EyeWatch not only overcomes the Big smartwatch problem: occlusion and fat finger problem, but also enables more powerful and natural interaction such as drawing a symbol quickly to open an application, or intuitively handwriting on back of hand to input message. Our proof-of-concept implementation consists of a micro-camera and an IR laser line generator on the smart watch, and computer vision algorithms are used to calculate the finger’s position on the back of hand.</h3>                                                       
                        </div>
                    </div>
                    <button type="button" class="btn btn-primary" data-dismiss="modal"><i class="fa fa-times"></i> Close Project</button>
                </div>
            </div>
        </div>
    </div>

    <!-- Project Modal 7 -->

    <div class="work-modal modal fade" id="workModal7" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="modal-body">                                      
                    <div class="row">
                        <div class="col-sm-8 col-sm-offset-2 text-left">
                            <div class="modal-title text-center">
                                <h2>The Incredible Shrinking Adventure</h2>
                            </div>
                            <div class="item active embed-responsive embed-responsive-16by9 video-wraper">
                                <iframe src="https://www.youtube.com/embed/UPvq2fLKTsQ" frameborder="0" allowfullscreen></iframe>                                
                            </div>                                                                                                  
                            <h4><i class="fa fa-users"></i><span class="bold">Cheng-Yao Wang,</span>Min-Chieh Hsiu, Chin-Yu Chien, Shuo Yang</h4>
                            <h3>ACM UIST 2014 Student Innovation Contest</h3> 
                            <h3>Imagining you were shrunk, you can explore your big house, play with big pets and family. To fulfill the imagination and provide users with incredible shrinking adventures, we use a robotic car and a google cardboard which turns a smartphone into a VR headset. We build a robotic car and attach a smartphone on the pan/tilt servo bracket. stereo images are generated from smartphone’s camera and are streamed to the other smartphone inside of the google cardboard. When users see the world through the smartphone on robotic car, they feel they were shrunk.</h3>                                                       
                        </div>
                    </div>
                    <button type="button" class="btn btn-primary" data-dismiss="modal"><i class="fa fa-times"></i> Close Project</button>
                </div>
            </div>
        </div>
    </div>

       

    <!-- jQuery Version 1.11.0 -->
    <script src="js/jquery-1.11.0.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="js/agency.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
    <script src="js/classie.js"></script>
    <script src="js/cbpAnimatedHeader.js"></script>

    <!-- Custom Theme JavaScript -->    
    <script src="js/ekko-lightbox.min.js"></script>
    <script src="js/jquery.lazyload.min.js"></script>
    <script src="slick/slick.min.js"></script>


    <script type="text/javascript">
        
        $(function() {
            
            $(document).delegate('*[data-toggle="lightbox"]', 'click', function(event) {
                event.preventDefault();
                $(this).ekkoLightbox();
            }); 
            
            $('#myHeader').lazyload({
                effect : "fadeIn"
            });

            $('img.lazy').lazyload({
                effect : "fadeIn"
            });

            $('.my-slide').slick({
              dots: true,
              infinite: true,
              speed: 500,
              fade: true,
              arrows: false,              
              cssEase: 'linear',
              lazyLoad: 'progressive'
            });            

            //stop you tube player when bootstrap modal was hiden
            $('.modal').on('hide.bs.modal', function () {
               $(this).find('iframe').attr("src", $(this).find('iframe').attr("src"));
            })    
        });

    </script>


</body>

</html>
